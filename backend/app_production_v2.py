"""
PRODUCTION BACKEND - Study Assistant API
Includes exam-grade flashcard system with fallback to mock data
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn
import os
from typing import Optional, List, Dict, Any, Literal
from datetime import datetime
import json

app = FastAPI(
    title="Study Assistant API - Production",
    description="Exam-grade flashcard system with AI intelligence",
    version="2.0.0"
)

# CORS Configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure with your Netlify domain in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== REQUEST/RESPONSE MODELS ====================

class AskRequest(BaseModel):
    question: str = Field(..., min_length=1)
    use_memory: bool = True
    top_k: int = Field(5, ge=1, le=20)

class AskResponse(BaseModel):
    answer: str
    context_used: bool
    sources_count: int
    sources: Optional[List[Dict[str, Any]]] = None

class FlashcardRequest(BaseModel):
    text: Optional[str] = None
    topic: Optional[str] = None
    num_cards: int = Field(5, ge=1, le=20)
    use_memory: bool = True

class Flashcard(BaseModel):
    question: str
    answer: str

class FlashcardResponse(BaseModel):
    flashcards: List[Flashcard]
    count: int
    source: str

class ExamGradeFlashcardRequest(BaseModel):
    text: Optional[str] = None
    topic: str = Field(..., min_length=1)
    num_cards: int = Field(10, ge=1, le=50)
    difficulty: Literal["beginner", "intermediate", "advanced", "expert"] = "intermediate"
    use_memory: bool = True
    user_mistakes: Optional[List[str]] = None
    force_card_types: Optional[List[str]] = None

class ExamGradeFlashcardData(BaseModel):
    id: str
    type: str
    question: str
    answer: str
    difficulty: str
    topic: str
    subtopic: Optional[str]
    source: str
    exam_relevance: int
    key_terms: List[str]
    created_at: str
    mistake_prone: bool

class ExamGradeFlashcardResponse(BaseModel):
    flashcards: List[ExamGradeFlashcardData]
    count: int
    source: str
    card_type_distribution: Dict[str, int]
    difficulty: str

class TrapCardRequest(BaseModel):
    mistakes: List[Dict[str, str]]
    topic: str
    num_cards: int = Field(5, ge=1, le=20)

class ExamSimulationRequest(BaseModel):
    topic: str
    subtopics: List[str]
    exam_format: Literal["multiple_choice", "short_answer", "essay"] = "multiple_choice"
    num_cards: int = Field(10, ge=1, le=30)

class StudyPlanRequest(BaseModel):
    subject: str = Field(..., min_length=1)
    days: int = Field(..., ge=1, le=365)
    current_knowledge: Optional[str] = None
    retrieve_materials: bool = True

class StudyPlanResponse(BaseModel):
    plan: str
    subject: str
    days: int
    materials_used: int

class IntelligentAskRequest(BaseModel):
    message: str
    mode: Optional[str] = None
    context: Optional[Dict[str, Any]] = None
    use_memory: bool = True
    skip_intervention: bool = False

class IntelligentAskResponse(BaseModel):
    mode: str
    answer: str
    context_used: bool
    intervention: Optional[Dict[str, Any]] = None
    suggestions: List[Dict[str, Any]]
    confidence: float

# ==================== MOCK DATA GENERATORS ====================

def generate_mock_exam_flashcards(topic: str, num_cards: int, difficulty: str) -> List[Dict[str, Any]]:
    """Generate mock exam-grade flashcards"""
    card_types = ["definition", "why", "how", "compare", "trap", "example", "exam"]
    flashcards = []
    
    for i in range(num_cards):
        card_type = card_types[i % len(card_types)]
        flashcard = {
            "id": f"card_{datetime.now().strftime('%Y%m%d%H%M%S')}_{i+1}",
            "type": card_type,
            "question": f"[{card_type.upper()}] Sample question about {topic} #{i+1}",
            "answer": f"This is a comprehensive {difficulty}-level answer about {topic}. In production with Ollama, this would be an intelligent, exam-grade response generated by Qwen2.5.",
            "difficulty": difficulty,
            "topic": topic,
            "subtopic": f"Subtopic {(i % 3) + 1}",
            "source": "mock_generation",
            "exam_relevance": min(10, 5 + (i % 6)),
            "key_terms": [topic.lower(), f"concept_{i+1}", "study"],
            "created_at": datetime.now().isoformat(),
            "mistake_prone": i % 5 == 0
        }
        flashcards.append(flashcard)
    
    return flashcards

def get_card_type_distribution(flashcards: List[Dict[str, Any]]) -> Dict[str, int]:
    """Calculate card type distribution"""
    distribution = {}
    for card in flashcards:
        card_type = card["type"]
        distribution[card_type] = distribution.get(card_type, 0) + 1
    return distribution

# ==================== HEALTH CHECK ====================

@app.get("/health")
async def health_check():
    """Check API health status"""
    return {
        "backend": "online",
        "status": "healthy",
        "services": {
            "api": {
                "status": "healthy",
                "version": "2.0.0",
                "features": ["exam-grade-flashcards", "ai-intelligence", "srs-engine"]
            },
            "qdrant": {
                "status": "mock",
                "message": "Using mock data. Configure QDRANT_URL for vector search."
            },
            "ollama": {
                "status": "mock",
                "message": "Using mock data. Configure OLLAMA_URL for AI generation."
            },
            "whisper": {
                "status": "disabled",
                "message": "Not configured in production environment."
            }
        },
        "mode": "PRODUCTION_MOCK",
        "note": "API is functional with mock data. Configure AI services for full features."
    }

# ==================== AI ENDPOINTS ====================

@app.post("/ai/ask", response_model=AskResponse)
async def ask_question(request: AskRequest):
    """Answer questions using AI (mock implementation)"""
    return {
        "answer": f"**Production Mode**: I understand your question: '{request.question}'. In production with Ollama and Qdrant configured, this would provide an intelligent, context-aware answer using RAG (Retrieval Augmented Generation). Configure OLLAMA_URL and QDRANT_URL environment variables to enable full AI capabilities.",
        "context_used": False,
        "sources_count": 0,
        "sources": None
    }

@app.post("/ai/flashcards/generate", response_model=FlashcardResponse)
async def generate_flashcards(request: FlashcardRequest):
    """Generate basic flashcards (legacy endpoint)"""
    topic = request.topic or "General Study"
    flashcards = [
        {
            "question": f"Question {i+1} about {topic}",
            "answer": f"Answer {i+1} for {topic}. Configure Ollama for intelligent generation."
        }
        for i in range(request.num_cards)
    ]
    
    return {
        "flashcards": flashcards,
        "count": len(flashcards),
        "source": "mock_generation"
    }

@app.post("/ai/flashcards/exam-grade", response_model=ExamGradeFlashcardResponse)
async def generate_exam_grade_flashcards(request: ExamGradeFlashcardRequest):
    """Generate exam-grade flashcards with 7 card types"""
    flashcards = generate_mock_exam_flashcards(request.topic, request.num_cards, request.difficulty)
    distribution = get_card_type_distribution(flashcards)
    
    return {
        "flashcards": flashcards,
        "count": len(flashcards),
        "source": "mock_generation_exam_grade",
        "card_type_distribution": distribution,
        "difficulty": request.difficulty
    }

@app.post("/ai/flashcards/trap-cards")
async def generate_trap_cards(request: TrapCardRequest):
    """Generate TRAP cards from mistakes"""
    trap_cards = []
    
    for i, mistake in enumerate(request.mistakes[:request.num_cards]):
        trap_card = {
            "id": f"trap_{datetime.now().strftime('%Y%m%d%H%M%S')}_{i+1}",
            "type": "trap",
            "question": f"What's a common mistake about {mistake.get('question', 'this topic')}?",
            "answer": f"Many students incorrectly think: '{mistake.get('wrong_answer', 'wrong concept')}'. Actually, the correct understanding is: '{mistake.get('correct_answer', 'correct concept')}'. This is a critical distinction for exams.",
            "difficulty": "intermediate",
            "topic": request.topic,
            "subtopic": None,
            "source": "mistake_analysis",
            "exam_relevance": 9,
            "key_terms": ["misconception", "trap"],
            "created_at": datetime.now().isoformat(),
            "mistake_prone": True
        }
        trap_cards.append(trap_card)
    
    return {
        "flashcards": trap_cards,
        "count": len(trap_cards),
        "source": "mistake_analysis",
        "all_trap_type": True
    }

@app.post("/ai/flashcards/exam-simulation")
async def generate_exam_simulation(request: ExamSimulationRequest):
    """Generate exam simulation cards"""
    exam_cards = []
    
    for i, subtopic in enumerate(request.subtopics):
        for j in range(max(1, request.num_cards // len(request.subtopics))):
            exam_card = {
                "id": f"exam_{datetime.now().strftime('%Y%m%d%H%M%S')}_{i}_{j}",
                "type": "exam",
                "question": f"[{request.exam_format.upper()}] Exam question on {subtopic} of {request.topic}",
                "answer": f"Comprehensive exam-style answer for {subtopic}. This would be a detailed, exam-ready response in production with Ollama.",
                "difficulty": "advanced",
                "topic": request.topic,
                "subtopic": subtopic,
                "source": "exam_simulation",
                "exam_relevance": 10,
                "key_terms": [subtopic.lower(), request.topic.lower()],
                "created_at": datetime.now().isoformat(),
                "mistake_prone": False
            }
            exam_cards.append(exam_card)
            if len(exam_cards) >= request.num_cards:
                break
        if len(exam_cards) >= request.num_cards:
            break
    
    return {
        "flashcards": exam_cards[:request.num_cards],
        "count": min(len(exam_cards), request.num_cards),
        "source": "exam_simulation",
        "exam_format": request.exam_format,
        "subtopics_covered": request.subtopics
    }

@app.post("/ai/plan/create", response_model=StudyPlanResponse)
async def create_study_plan(request: StudyPlanRequest):
    """Generate study plan"""
    plan_text = f"""
# Study Plan: {request.subject}
Duration: {request.days} days

## Overview
This is a {request.days}-day study plan for {request.subject}.

## Daily Schedule
{'- Day 1-7: Foundations and core concepts\n' if request.days >= 7 else ''}
{'- Day 8-14: Advanced topics and practice\n' if request.days >= 14 else ''}
{'- Day 15-21: Review and reinforcement\n' if request.days >= 21 else ''}
{'- Final days: Exam preparation and practice tests\n' if request.days >= 7 else ''}

## Recommendations
- Study for 2-3 hours per day
- Take regular breaks (Pomodoro technique)
- Review flashcards daily using SRS
- Practice with past papers

*Note: Configure Ollama with Mixtral for personalized, AI-generated study plans.*
    """
    
    return {
        "plan": plan_text.strip(),
        "subject": request.subject,
        "days": request.days,
        "materials_used": 0
    }

@app.post("/ai/intelligent-ask", response_model=IntelligentAskResponse)
async def intelligent_ask(request: IntelligentAskRequest):
    """Intelligent AI with mode detection and interventions"""
    detected_mode = request.mode or "explain"
    
    return {
        "mode": detected_mode,
        "answer": f"**Intelligent Response ({detected_mode} mode)**: {request.message}\n\nIn production with Ollama, this would provide context-aware, mode-specific responses with RAG and intelligent interventions.",
        "context_used": False,
        "intervention": None,
        "suggestions": [
            {
                "id": "sug_1",
                "message": "Try generating exam-grade flashcards on this topic",
                "action": "Generate flashcards",
                "priority": "normal",
                "icon": "sparkles"
            }
        ],
        "confidence": 0.85
    }

@app.get("/ai/health")
async def ai_health():
    """AI service health check"""
    return {
        "status": "healthy",
        "service": "ai",
        "ollama": "mock_mode",
        "memory": "mock_mode",
        "note": "Using mock data. Configure AI services for full functionality."
    }

# ==================== ROOT ENDPOINT ====================

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "service": "Study Assistant API",
        "version": "2.0.0",
        "status": "running",
        "features": [
            "Exam-grade flashcards (7 card types)",
            "Advanced SRS engine",
            "Mistake-driven TRAP cards",
            "AI intelligence system",
            "Study plan generation"
        ],
        "endpoints": {
            "health": "/health",
            "docs": "/docs",
            "flashcards_exam_grade": "/ai/flashcards/exam-grade",
            "flashcards_trap": "/ai/flashcards/trap-cards",
            "flashcards_exam_sim": "/ai/flashcards/exam-simulation",
            "ai_ask": "/ai/ask",
            "ai_intelligent": "/ai/intelligent-ask",
            "study_plan": "/ai/plan/create"
        },
        "note": "Mock mode active. Configure OLLAMA_URL and QDRANT_URL for full AI features."
    }

# ==================== RUN SERVER ====================

if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(
        "app_production_v2:app",
        host="0.0.0.0",
        port=port,
        reload=False
    )
